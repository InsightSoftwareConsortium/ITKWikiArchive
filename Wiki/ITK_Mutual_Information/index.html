<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<meta name="generator" content="MediaWiki 1.13.1" />
		<meta name="keywords" content="ITK Mutual Information,ITK,ITK" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/Wiki/opensearch_desc.php" title="KitwarePublic (en)" />
		<link title="Creative Commons" type="application/rdf+xml" href="/Wiki/index.php?title=ITK_Mutual_Information&amp;action=creativecommons" rel="meta" />
		<link rel="copyright" href="http://creativecommons.org/licenses/by/2.5/" />
		<link rel="alternate" type="application/rss+xml" title="KitwarePublic RSS Feed" href="http://www.itk.org/Wiki/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="KitwarePublic Atom Feed" href="http://www.itk.org/Wiki/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>ITK Mutual Information - KitwarePublic</title>
		<style type="text/css" media="screen, projection">/*<![CDATA[*/
			@import "/Wiki/skins/common/shared.css?164";
			@import "/Wiki/skins/monobook/main.css?164";
		/*]]>*/</style>
		<link rel="stylesheet" type="text/css" media="print" href="/Wiki/skins/common/commonPrint.css?164" />
		<!--[if lt IE 5.5000]><style type="text/css">@import "/Wiki/skins/monobook/IE50Fixes.css?164";</style><![endif]-->
		<!--[if IE 5.5000]><style type="text/css">@import "/Wiki/skins/monobook/IE55Fixes.css?164";</style><![endif]-->
		<!--[if IE 6]><style type="text/css">@import "/Wiki/skins/monobook/IE60Fixes.css?164";</style><![endif]-->
		<!--[if IE 7]><style type="text/css">@import "/Wiki/skins/monobook/IE70Fixes.css?164";</style><![endif]-->
		<!--[if lt IE 7]><script type="text/javascript" src="/Wiki/skins/common/IEFixes.js?164"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->
		
		<script type= "text/javascript">/*<![CDATA[*/
var skin = "monobook";
var stylepath = "/Wiki/skins";
var wgArticlePath = "/Wiki/$1";
var wgScriptPath = "/Wiki";
var wgScript = "/Wiki/index.php";
var wgVariantArticlePath = false;
var wgActionPaths = [];
var wgServer = "http://www.itk.org";
var wgCanonicalNamespace = "";
var wgCanonicalSpecialPageName = false;
var wgNamespaceNumber = 0;
var wgPageName = "ITK_Mutual_Information";
var wgTitle = "ITK Mutual Information";
var wgAction = "view";
var wgArticleId = "626";
var wgIsArticle = true;
var wgUserName = null;
var wgUserGroups = null;
var wgUserLanguage = "en";
var wgContentLanguage = "en";
var wgBreakFrames = false;
var wgCurRevisionId = "4151";
var wgVersion = "1.13.1";
var wgEnableAPI = true;
var wgEnableWriteAPI = false;
var wgRestrictionEdit = [];
var wgRestrictionMove = [];
/*]]>*/</script>
                
		<script type="text/javascript" src="/Wiki/skins/common/wikibits.js?164"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/Wiki/skins/common/ajax.js?164"></script>
		<script type="text/javascript" src="/Wiki/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
		<style type="text/css">/*<![CDATA[*/
@import "/Wiki/index.php?title=MediaWiki:Common.css&usemsgcache=yes&action=raw&ctype=text/css&smaxage=18000";
@import "/Wiki/index.php?title=MediaWiki:Monobook.css&usemsgcache=yes&action=raw&ctype=text/css&smaxage=18000";
@import "/Wiki/index.php?title=-&action=raw&gen=css&maxage=18000";
/*]]>*/</style>
	</head>
<body class="mediawiki ns-0 ltr page-ITK_Mutual_Information"><!-- start content -->
			<a name="What_is_Mutual_Information"></a><h2> <span class="mw-headline"> What is Mutual Information </span></h2>
<p>When you consider the pixel values of images A and B to be random variables, "a" and "b"; and estimate the entropy of their distributions you get
</p><p><img class="tex" alt="&#10;    H(A) = - \sum_{i=0}^M { p(a_i) \cdot log_2( p(a_i) ) }&#10;" src="/Wiki/images/math/6/d/d/6dd7f7bf76639f3cf12edbe8ef87b3f2.png" />
</p><p><img class="tex" alt="&#10;    H(B) = - \sum_{i=0}^M { p(b_i) \cdot log_2( p(b_i) ) }&#10;" src="/Wiki/images/math/9/7/3/973e1146f4a3b6aaa9ca01fd795d7721.png" />
</p><p>Note that log2 is the logarithm in base two, not the natural logarigthm that unfortunately is commonly used.
</p><p>When you use log2(), the Units of entropy are "bit"s. It is again unfortunate that the usage of "bit" as unit of information measurement has been distorted in order to become the a symbol for binary encoding, o a unit of measurement for raw capacity of memory storage (along with its derivatives the byte, KiloByte, MegaByte... )
</p><p>A digital image whose pixels are encoded in pixels of M bits, can have 2^M different grayscale values in a pixel and therfore its entropy can go up to the maximum theoretical value of log2(2^M) which, not coincidentally is equal to M.
</p><p>In other words, if you compute the entropy of an image with PixelType unsigned char, whose pixels have grayscale values following a uniform distribution, the maximum value that you can get is "8", and if you want to be formal, you should mention the units and say:
</p>
<pre>  The Entropy of this image is:    "8 bits"
</pre>
<p>In practice, of course you get lower values. For example the Entropy of the well known Lena image (the cropped version that is politicaly correct) is
</p>
<pre>         Lena Entropy  = 7.44 bits
</pre>
<p><br />
Now if you consider the mutual information measure, you have the following situation:
</p><p><br />
</p>
<pre>     Mutual Information = H(A) + H(B) - H(A,B)
</pre>
<pre>             MI(A,B)     = H(A) + H(B) - H(A,B)
</pre>
<p>In general, both H(A) and H(B) are bounded between [0:M] where "M" is the number of bits used for encoding their pixels.
</p>
<pre>      H(A,B) is in theory bounded in [0:M2]
</pre>
<p><br />
Note that if you use histograms with *less* bins than the actual digital encoding of the image, then your estimation of Entropy is bounded by the number of bins in your Histogram.&nbsp;!
</p><p>For example if you use a histogram with 20 bins instead of 256 in order to estimate Lena's Entropy you will not get 7.44 bits, but only
</p>
<pre>                  3.99 bits
</pre>
<p>that reflects the fact that by quantizing the gray scale values in larger bins you lose information from the original image.
</p><p><br />
</p><p>For the particular case of Self-similarity, the entropies H(A) and H(B) are expected to be pretty much the same. Their difference arise only from interpolation errors and from the eventual effect of one image having corners outside of the extent of the other (e.g. if the image is rotated).
</p><p>So, in general Mutual Information will give you
</p>
<pre>          MI(A,T(A)) = H(A) + H(T(A)) - H(A,T(A))
</pre>
<p>Where T(A) is the transformed version of A. E.g. under a translation, or rotation, or affine transform.
</p><p>If T = Identity and the interpolator is not approximating, your measure of Mutual Information becomes
</p>
<pre>            MI(A,A) = 2 H(A) - H(A,A)
</pre>
<p>and the joint entropy H(A,A) happens to be equal to the entropy of the single image H(A), therefore the expected value of Mutual Information is equal to the image Entropy (of course measured in bits).
</p>
<pre>                   MI(A,A) = H(A) bits
</pre>
<p>That means that if you evaluate the Mutual Information measure between Lena and itself, you should get
</p>
<pre>                     7.44 bits
</pre>
<p><br />
Note that the reason why the measure of Mutual Information is reported as a negative number in ITK is because traditionally that has been used as a cost function for minimization.
</p><p>However, in principle, Mutual information should be reported in the range [0,H(A)], where zero corresponds to two totally uncorrelated images and H(A) corresponds to perfectly correlated images, case in which H(A)=H(B).
</p><p><br />
To summarize, note that ITK is not using log2() but just log(), and note that the measure is reported as a negative number.
</p><p>We just added a simple example for computing the Entropy of the pixel value distribution of an image to the directory:
</p>
<pre>      Insight/Examples/Statistics/
                         ImageEntropy1.cxx
</pre>
<p>You may find interesting to play with this example. E.g. you should try the effect of changing the number of bins in the histogram.
</p><p><br />
</p>
<hr />
<table width="100%" border="0" cellspacing="0" cellpadding="0">

<tr>
<td bgcolor="#F9F9F9"> <center><small>ITK: &#91;<a href="/Wiki/ITK" title="ITK">Welcome</a> | <a href="/Wiki/Category:ITK" title="Category:ITK">Site Map</a>&#93;</small></center>
</td></tr></table>

<!-- 
NewPP limit report
Preprocessor node count: 15/1000000
Post-expand include size: 222/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->

<!-- Saved in parser cache with key KitwarePublicWikiDB:pcache:idhash:626-0!1!0!!en!2!edit=0 and timestamp 20100109143120 -->
<div class="printfooter">
Retrieved from "<a href="http://www.itk.org/Wiki/ITK_Mutual_Information">http://www.itk.org/Wiki/ITK_Mutual_Information</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/Wiki/Special:Categories" title="Special:Categories">Category</a>:&#32;<span dir='ltr'><a href="/Wiki/Category:ITK" title="Category:ITK">ITK</a></span></div></div>			<!-- end content --></body></html>
